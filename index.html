<!DOCTYPE html>
<html lang="en">
<head>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-84629802-4"></script>
	<script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'UA-84629802-4');
	</script>

	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">

	<meta name="description" content="">
	<meta name="author" content="">
	<meta name="keywords" content="EPOS, 6D object pose, pose estimation, symmetry" />

	<title>Neural Correspondence Field for Object Pose Estimation</title>

	<link href="css/bootstrap.min.css" rel="stylesheet">
<link href="css/style.css" rel="stylesheet">
</head>

<body>
<div class="container">

<nav class="navbar">
	<div class="title">
		<h1>Neural Correspondence Field for Object Pose Estimation</h1>
		<p class="authors">
		<a href="https://linhuang17.github.io">Lin Huang</a>&nbsp;&nbsp;&nbsp;
		<a href="http://www.hodan.xyz">Tomáš Hodaň</a>&nbsp;&nbsp;&nbsp;
		<a href="https://scholar.google.nl/citations?user=eUAgpwkAAAAJ&hl=en">Lingni Ma</a>&nbsp;&nbsp;&nbsp;
		<a href="https://lg-zhang.github.io">Linguang Zhang</a>&nbsp;&nbsp;&nbsp;
		<a href="http://cse.msu.edu/~tranluan/">Luan Tran</a>&nbsp;&nbsp;&nbsp;
		<a href="https://scholar.google.com/citations?user=aN-lQ0sAAAAJ&hl=en">Christopher Twigg</a>&nbsp;&nbsp;&nbsp;<br>
		<a href="http://media.ee.ntu.edu.tw/personal/pcwu/">Po-Chen Wu</a>&nbsp;&nbsp;&nbsp;
		<a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>&nbsp;&nbsp;&nbsp;
		<a href="https://scholar.google.com/citations?user=9HoiYnYAAAAJ&hl=en">Cem Keskin</a>&nbsp;&nbsp;&nbsp;
		<a href="http://people.csail.mit.edu/rywang/">Robert Wang</a>
		</p>
	</div>

    <div class="logos">
		<a href="https://www.buffalo.edu"><img src="img/UB_Stacked_SUNY_Small.png" alt="University at Buffalo" /></a>
		<a href="https://about.facebook.com/realitylabs/"><img src="img/RL_Logo_White.png" alt="Reality Labs at Meta" /></a>
	</div>

	<div class="end"></div>
</nav>

<div class="block pub_links">
	<a href="https://linhuang17.github.io/NCF/" class="pub_link">PAPER</a>
	<a href="https://github.com/LinHuang17/NCF-code" class="pub_link">CODE</a>
	<!-- <a href="https://linhuang17.github.io/NCF/" class="pub_link">VIDEO</a> -->
	<!-- <a href="https://linhuang17.github.io/NCF/" class="pub_link">DEMO</a> -->
	<a href="https://linhuang17.github.io/NCF/" class="pub_link">BIB</a>
	<span class="published_at">Published at <a href="https://eccv2022.ecva.net">ECCV 2022</a></span>
	<div class="end"></div>
</div>

<div class="block block_with_video">
	<h2 id="abstract">Abstract</h2>
	<p>
	We propose a method for estimating the 6DoF pose of a rigid object with an available 3D model from a single RGB image. Unlike classical correspondence-based methods which predict 3D object coordinates at pixels of the input image, the proposed method predicts 3D object coordinates at 3D query points sampled in the camera frustum. The move from pixels to 3D points, which is inspired by recent PIFu-style methods for 3D reconstruction, enables reasoning about the whole object, including its (self-)occluded parts. For a 3D query point associated with a pixel-aligned image feature, we train a fully-connected neural network to predict: (i) the corresponding 3D object coordinates, and (ii) the signed distance to the object surface, with the first defined only for query points in the surface vicinity. We call the mapping realized by this network as Neural Correspondence Field. The object pose is then robustly estimated from the predicted 3D-3D correspondences by the Kabsch-RANSAC algorithm. The proposed method achieves state-of-the-art results on three BOP datasets and is shown superior especially in challenging cases with occlusion.
	</p>

	<!-- <div class="embedded_video">
		<iframe width="650" height="500" src="https://www.youtube.com/embed/OXjG0YPqLnE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	</div> -->
</div>

<!-- <div class="block">
	<h2 id="code">Code and pre-trained models</h2>
	<p>Code and models evaluated in the <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Hodan_EPOS_Estimating_6D_Pose_of_Objects_With_Symmetries_CVPR_2020_paper.pdf">CVPR 2020 paper</a> and in the <a href="https://arxiv.org/pdf/2009.07378.pdf">BOP Challenge 2020</a> are available on <a href="https://github.com/thodan/epos">GitHub</a>.</p>
</div>

<div class="block">
	<h2 id="demo">Real-world demo</h2>
	<p>EPOS was applied frame by frame (no temporal consistency was enforced) to a video of YCB objects captured by a cell phone. The RGB input image is on the left and the 3D object models rendered in the estimated 6D poses on the right. All pose estimates with at least 100 inlier correspondences are shown. Each pose estimate is annotated with the object ID and the number of inlier correspondences.</p>
	
	<div class="embedded_video">
		<iframe width="800" height="300" src="https://www.youtube.com/embed/7YOfcayv67w" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	</div>
</div> -->

</div>

<!-- Bootstrap core JavaScript -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="js/vendor/jquery.min.js"><\/script>')</script>
<script src="js/bootstrap.min.js"></script>

</body>
</html>
